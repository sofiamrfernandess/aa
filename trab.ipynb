{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as ni\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de pixels na imagem: 8388608\n",
      "(256, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "def load(fname, affine=False, header=False):\n",
    "    if header:\n",
    "        affine = True\n",
    "    \n",
    "    data = ni.load(fname)\n",
    "    if affine:\n",
    "        affine_info = data.affine\n",
    "    if header:\n",
    "        header_info = data.header\n",
    "        header_info.set_data_dtype(np.uint8)\n",
    "    data = np.asarray(data.get_fdata(), dtype=np.float32, order='C')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "mri_file = '/Users/sofiaribas/Downloads/SEIM/IBSR_nifti_stripped/IBSR_{subj}/IBSR_{subj}_ana.nii.gz'\n",
    "mri_strip_file = '/Users/sofiaribas/Downloads/SEIM/IBSR_nifti_stripped/IBSR_{subj}/IBSR_{subj}_ana_strip.nii.gz'\n",
    "seg_file = '/Users/sofiaribas/Downloads/SEIM/IBSR_nifti_stripped/IBSR_{subj}/IBSR_{subj}_seg_ana.nii.gz'\n",
    "seg_TRI_file = '/Users/sofiaribas/Downloads/SEIM/IBSR_nifti_stripped/IBSR_{subj}/IBSR_{subj}_segTRI_ana.nii.gz'\n",
    "seg_TRI_fill_file = '/Users/sofiaribas/Downloads/SEIM/IBSR_nifti_stripped/IBSR_{subj}/IBSR_{subj}_segTRI_fill_ana.nii.gz'\n",
    "mri_mask_file =  '/Users/sofiaribas/Downloads/SEIM/IBSR_nifti_stripped/IBSR_{subj}/IBSR_{subj}_ana_brainmask.nii.gz'\n",
    "\n",
    "\n",
    "\n",
    "mri = load(mri_strip_file.format(subj='01'))\n",
    "altura, largura, profundidade,_ = mri.shape\n",
    "numero_de_pixels = altura * largura * profundidade\n",
    "print(\"Número de pixels na imagem:\", numero_de_pixels)\n",
    "\n",
    "mri_mask = load(mri_mask_file.format(subj='01'))\n",
    "mri_m = load(seg_TRI_fill_file.format(subj='01'))\n",
    "print(mri_mask.shape)\n",
    "#_, ax = plt.subplots(1, 3, figsize=(8, 8))\n",
    "\n",
    "#ax[0].imshow(mri[:, :, 100])\n",
    "#ax[1].imshow((mri_mask[:,:,100]))\n",
    "#ax[2].imshow(mri_m[:,:,130])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização -> Normalização por percentil, de modo a eliminar outliers. As intensidades estão entre 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_normalize(image, mask, lower_percentile=1, upper_percentile=99):\n",
    "    \n",
    "    brain = image[mask > 0]\n",
    "    \n",
    "    lower_bound = np.percentile(brain, lower_percentile)\n",
    "    upper_bound = np.percentile(brain, upper_percentile)\n",
    "    \n",
    "    normalized_image = np.clip(image, lower_bound, upper_bound)\n",
    "    \n",
    "    normalized_image = (normalized_image - lower_bound) / (upper_bound - lower_bound)\n",
    "    \n",
    "    return normalized_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformação Afim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_affine_transform(image, lb_new, ub_new):\n",
    "\n",
    "    transformed_image = image * (ub_new - lb_new) + lb_new\n",
    "    \n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "\n",
    "#normalized_image2 = percentile_normalize(mri[:,:,120], mri_mask[:,:,120])\n",
    "#affim = apply_affine_transform(normalized_image2, 4, 7)\n",
    "\n",
    "#_, ax = plt.subplots(1, 2, figsize=(8, 8))\n",
    "\n",
    "#ax[0].imshow(normalized_image2)\n",
    "#ax[1].imshow(affim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_transformation(image):\n",
    "\n",
    "    return np.exp(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import convolve\n",
    "\n",
    "def local_intensity_mean(img):\n",
    "    \n",
    "    kernel1 = np.ones((3, 3, 3, 1))/(3*3*3)\n",
    "    kernel2 = np.ones((9, 9, 9, 1))/(9*9*9)\n",
    "\n",
    "    convolved_image1 = convolve(img, kernel1, mode='constant')\n",
    "    convolved_image2 = convolve(img, kernel2, mode='constant')\n",
    "\n",
    "    return convolved_image1, convolved_image2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import sobel\n",
    "\n",
    "def gradient_magnitude(image):\n",
    "    # Compute the Sobel gradients along each axis\n",
    "    gradient_x = sobel(image, axis=0)\n",
    "    gradient_y = sobel(image, axis=1)\n",
    "    gradient_z = sobel(image, axis=2)\n",
    "\n",
    "    # Compute the gradient magnitude\n",
    "    gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2 + gradient_z**2)\n",
    "\n",
    "    return gradient_magnitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m sub \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m03\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m04\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m07\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m08\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m11\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m14\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m15\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m17\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m18\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sub: \n\u001b[0;32m----> 9\u001b[0m     mri \u001b[38;5;241m=\u001b[39m load(mri_strip_file\u001b[38;5;241m.\u001b[39mformat(subj\u001b[38;5;241m=\u001b[39mi))\n\u001b[1;32m     10\u001b[0m     mri_mask \u001b[38;5;241m=\u001b[39m load(mri_mask_file\u001b[38;5;241m.\u001b[39mformat(subj\u001b[38;5;241m=\u001b[39mi))\n\u001b[1;32m     11\u001b[0m     mri_s \u001b[38;5;241m=\u001b[39m load(seg_TRI_fill_file\u001b[38;5;241m.\u001b[39mformat(subj\u001b[38;5;241m=\u001b[39mi))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "images = [] \n",
    "masks = []\n",
    "segmentadas = []\n",
    "\n",
    "sub = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17' ,'18']\n",
    "\n",
    "for i in sub: \n",
    "    \n",
    "    mri = load(mri_strip_file.format(subj=i))\n",
    "    mri_mask = load(mri_mask_file.format(subj=i))\n",
    "    mri_s = load(seg_TRI_fill_file.format(subj=i))\n",
    "\n",
    "    images.append(mri)\n",
    "    masks.append(mri_mask)\n",
    "    segmentadas.append(mri_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar imagens\n",
    "\n",
    "NOTA: A numeração das imagens começa no 0 em vez de começar no 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (ima, mas) in enumerate(zip(images, masks), start=1):\n",
    "    exp = exponential_transformation(apply_affine_transform(percentile_normalize(ima, mas), 4, 7))\n",
    "    filename = f\"/Users/sofiaribas/Downloads/SEIM/exp_1/exp_{i:02d}.joblib\"\n",
    "    joblib.dump(exp, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (ima, mas) in enumerate(zip(images, masks), start=1):\n",
    "    norm = percentile_normalize(ima,mas)\n",
    "    filename = f\"/Users/sofiaribas/Downloads/SEIM/norm_1/norm_{i:02d}.joblib\" \n",
    "    joblib.dump(norm, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (ima, mas) in enumerate(zip(images, masks), start=1):\n",
    "    local1, local2 = local_intensity_mean(ima)\n",
    "    grad = gradient_magnitude(ima)\n",
    "\n",
    "    filename_local1 = f\"/Users/sofiaribas/Downloads/SEIM/local_1/local_{i:02d}_3.joblib\"\n",
    "    filename_local2 = f\"/Users/sofiaribas/Downloads/SEIM/local_1/local_{i:02d}_9.joblib\"\n",
    "    filename_grad = f\"/Users/sofiaribas/Downloads/SEIM/grad_1/grad_{i:02d}.joblib\"\n",
    "    \n",
    "    joblib.dump(local1, filename_local1)\n",
    "    joblib.dump(local2, filename_local2)\n",
    "    joblib.dump(grad, filename_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar a Matriz das Features \n",
    "\n",
    "Para cada sujeito selecionamos 1 000 000 pontos e a cada ponto aplicamos as 5 features. Para cada sujeito são selecionados voxeis diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def extract_features(person_id):\n",
    "\n",
    "    norm_path = f\"/Users/sofiaribas/Downloads/SEIM/norm/norm_{person_id}.joblib\"\n",
    "    exp_path = f\"/Users/sofiaribas/Downloads/SEIM/exp/exp_{person_id}.joblib\"\n",
    "    grad_path = f\"/Users/sofiaribas/Downloads/SEIM/grad/grad_{person_id}.joblib\"\n",
    "    local3_path = f\"/Users/sofiaribas/Downloads/SEIM/local/local_{person_id}_3.joblib\"\n",
    "    local9_path = f\"/Users/sofiaribas/Downloads/SEIM/local/local_{person_id}_9.joblib\"\n",
    "    image_segmented = f\"/Users/sofiaribas/Downloads/SEIM/IBSR_nifti_stripped/IBSR_{person_id}/IBSR_{person_id}_segTRI_fill_ana.nii.gz\"\n",
    "    mri_mask_path = f\"/Users/sofiaribas/Downloads/SEIM/IBSR_nifti_stripped/IBSR_{person_id}/IBSR_{person_id}_ana_brainmask.nii.gz\"\n",
    "\n",
    "    imagem_norm = joblib.load(norm_path)\n",
    "    imagem_exp = joblib.load(exp_path)\n",
    "    imagem_grad = joblib.load(grad_path)\n",
    "    imagem_local3 = joblib.load(local3_path)\n",
    "    imagem_local9 = joblib.load(local9_path)\n",
    "    \n",
    "    segmented_data = nib.load(image_segmented).get_fdata()\n",
    "    mask = nib.load(mri_mask_path).get_fdata()\n",
    "    non_zero_coords = np.where(mask != 0)\n",
    "    \n",
    "    num_points = 800 \n",
    "    selected_indices = np.random.choice(len(non_zero_coords[0]), num_points, replace=False)\n",
    "    \n",
    "    extracted_matrix = np.zeros((num_points, 5))\n",
    "    pontos_seg = np.zeros((num_points, 1))\n",
    "\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        x, y, z = non_zero_coords[0][idx], non_zero_coords[1][idx], non_zero_coords[2][idx]\n",
    "\n",
    "        extracted_matrix[i, 0] = imagem_norm[x, y, z]\n",
    "        extracted_matrix[i, 1] = imagem_exp[x, y, z]\n",
    "        extracted_matrix[i, 2] = imagem_grad[x, y, z]\n",
    "        extracted_matrix[i, 3] = imagem_local3[x, y, z]\n",
    "        extracted_matrix[i, 4] = imagem_local9[x, y, z]\n",
    "\n",
    "        pontos_seg[i, 0] = segmented_data[x, y, z]\n",
    "        \n",
    "    return extracted_matrix, pontos_seg\n",
    "\n",
    "sub = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18']\n",
    "\n",
    "\n",
    "# Process data for each person\n",
    "for person_id in sub:\n",
    "    extracted_matrix, pontos_seg = extract_features(person_id)\n",
    "print(extracted_matrix.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: ver questão das labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565 706 329\n",
      "123 954 523\n",
      "Fold 1 - Dice: 1.0000\n",
      "106 1083 411\n",
      "130 992 478\n",
      "Fold 2 - Dice: 1.0000\n",
      "573 589 438\n",
      "126 949 525\n",
      "Fold 3 - Dice: 1.0000\n",
      "104 898 598\n",
      "148 880 572\n",
      "Fold 4 - Dice: 1.0000\n",
      "503 666 431\n",
      "115 902 583\n",
      "Fold 5 - Dice: 1.0000\n",
      "76 919 605\n",
      "127 947 526\n",
      "Fold 6 - Dice: 1.0000\n",
      "25 1152 423\n",
      "138 946 516\n",
      "Fold 7 - Dice: 1.0000\n",
      "47 1059 494\n",
      "152 937 511\n",
      "Fold 8 - Dice: 1.0000\n",
      "47 1067 486\n",
      "152 962 486\n",
      "Fold 9 - Dice: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from medpy.metric.binary import dc\n",
    "import random\n",
    "\n",
    "\n",
    "def train_svm(X_train, y_train, C, gamma):\n",
    "\n",
    "    svm = SVC(C=C, gamma=gamma, kernel='rbf')\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    return svm\n",
    "\n",
    "\n",
    "# Lista para armazenar os índices de treino e teste para cada fold\n",
    "indices = []\n",
    "\n",
    "# Dividir os dados em folds\n",
    "for i in range(9):  \n",
    "    test_start = i * 2  # Dois pacientes por fold\n",
    "    test_end = test_start + 2\n",
    "    \n",
    "    # Índices para teste\n",
    "    test_indices = list(range(test_start, test_end))\n",
    "    \n",
    "    # Índices para treino\n",
    "    train_indices = [idx for idx in range(18) if idx not in test_indices]\n",
    "    \n",
    "    # Randomly shuffle the training indices\n",
    "    random.shuffle(train_indices)\n",
    "    \n",
    "    # Split the shuffled training indices into training and validation\n",
    "    train_fold_size = len(train_indices) - 2\n",
    "    \n",
    "    train_fold_indices = train_indices[:train_fold_size]\n",
    "   \n",
    "    val_fold_indices = train_indices[train_fold_size:]\n",
    "   \n",
    "    indices.append((train_fold_indices, val_fold_indices, test_indices))\n",
    "#------------\n",
    "def dice(predict, val):\n",
    "    \n",
    "\n",
    "\n",
    "#------------\n",
    "\n",
    "# Loop de validação cruzada\n",
    "for fold, (train_indices, val_indices, test_indices) in enumerate(indices):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    # Coletar dados de treino e validação\n",
    "    for idx in train_indices:\n",
    "        person_id = sub[idx]\n",
    "        extracted_matrix, pontos_seg = extract_features(person_id)\n",
    "        X_train.append(extracted_matrix)\n",
    "        y_train.append(pontos_seg)\n",
    "        \n",
    "    for idx in val_indices:\n",
    "        person_id = sub[idx]\n",
    "        extracted_matrix, pontos_seg = extract_features(person_id)\n",
    "        X_val.append(extracted_matrix)\n",
    "        y_val.append(pontos_seg)\n",
    "    \n",
    "    # Concatenar e transformar dados\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = np.concatenate(y_train).ravel()\n",
    "    X_val = np.concatenate(X_val)\n",
    "    y_val = np.concatenate(y_val).ravel()\n",
    "\n",
    "    svm = train_svm(X_train, y_train, C=1, gamma='scale')\n",
    "    y_pred = svm.predict(X_val)\n",
    "    print(np.count_nonzero(y_pred==1), np.count_nonzero(y_pred==2), np.count_nonzero(y_pred==3))\n",
    "    print(np.count_nonzero(y_val==1), np.count_nonzero(y_val==2), np.count_nonzero(y_val==3))\n",
    "    dice = dc(y_pred==1, y_val==1)\n",
    "                       \n",
    "    print(f\"Fold {fold + 1} - Dice: {dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_dice = 0\n",
    "    best_C = None\n",
    "    best_gamma = None\n",
    "    \n",
    "    for C in [0.1, 1, 10]:\n",
    "        for gamma in [0.001, 0.01, 0.1]:\n",
    "            svm = train_svm(X_train, y_train, C, gamma)\n",
    "            y_pred = svm.predict(X_val)\n",
    "            \n",
    "            dice = dice_coefficient(y_val, y_pred)\n",
    "            \n",
    "            if dice > best_dice:\n",
    "                best_dice = dice\n",
    "                best_C = C\n",
    "                best_gamma = gamma\n",
    "                \n",
    "    print(f\"Fold {fold + 1} - Melhores hiperparâmetros: C={best_C}, gamma={best_gamma}, Dice={best_dice:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
